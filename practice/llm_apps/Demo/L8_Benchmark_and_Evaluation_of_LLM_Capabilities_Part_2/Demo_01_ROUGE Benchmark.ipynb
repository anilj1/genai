{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "NId_WJGCdDAD",
   "metadata": {
    "id": "NId_WJGCdDAD"
   },
   "source": [
    "# **Demo: ROUGE Benchmark**\n",
    "\n",
    "This demo is designed to read a PDF file and a summary of that file, and then compute the ROUGE scores for the summary by comparing it with the original document. The ROUGE scores provide a measure of the quality of the summary.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "*   Use the **SUMMARY.txt** generated from the **Demo: Text_Summarizer**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0Il1Sz9dQcm",
   "metadata": {
    "id": "j0Il1Sz9dQcm"
   },
   "source": [
    "### **Steps to Perform:**\n",
    "\n",
    "\n",
    "*   Step 1: Import the Necessary Libraries\n",
    "*   Step 2: Read the PDF File\n",
    "*   Step 3: Read the Summary File\n",
    "*   Step 4: Load the ROUGE Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EVIHyQY2eL8l",
   "metadata": {
    "id": "EVIHyQY2eL8l"
   },
   "source": [
    "### **Step 1: Import the Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3ede8e-cab4-4c3c-898b-714eef43e576",
   "metadata": {
    "id": "fd3ede8e-cab4-4c3c-898b-714eef43e576"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 23:12:39.477345: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-15 23:12:39.530465: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import PyPDF2\n",
    "from evaluate import load\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GqA0mAqXffCZ",
   "metadata": {
    "id": "GqA0mAqXffCZ"
   },
   "source": [
    "### **Step 2: Read the PDF File**\n",
    "\n",
    "*   Open the PDF file.\n",
    "*   Create a **PdfReader** object for the PDF file.\n",
    "*   Extract the text from each page of the PDF and concatenate it into a single string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24a902f-c255-4dc0-9047-39cd6550aa44",
   "metadata": {
    "id": "f24a902f-c255-4dc0-9047-39cd6550aa44"
   },
   "outputs": [],
   "source": [
    "# Define the PDF file path\n",
    "pdf_path = \"arxiv_impact_of_GENAI.pdf\"\n",
    "\n",
    "# Check if the PDF file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(f\"Error: PDF file '{pdf_path}' not found.\")\n",
    "\n",
    "# Read the PDF file\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    document_text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            document_text += page_text + \" \"\n",
    "        else:\n",
    "            print(f\" Warning: Could not extract text from a page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZLcKO7C8fx4O",
   "metadata": {
    "id": "ZLcKO7C8fx4O"
   },
   "source": [
    "### **Step 3: Read the Summary File**\n",
    "\n",
    "*   Open the summary file and read its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4Awcxb2Yf-o0",
   "metadata": {
    "id": "4Awcxb2Yf-o0"
   },
   "outputs": [],
   "source": [
    "# Define summary file paths\n",
    "human_summary_path = \"Summary.txt\"\n",
    "ai_summary_path = \"SUMMARY.txt\"\n",
    "\n",
    "# Check if summary files exist\n",
    "if not os.path.exists(human_summary_path):\n",
    "    raise FileNotFoundError(f\"Error: Human summary file '{human_summary_path}' not found.\")\n",
    "if not os.path.exists(ai_summary_path):\n",
    "    raise FileNotFoundError(f\"Error: AI summary file '{ai_summary_path}' not found.\")\n",
    "\n",
    "# Read summaries\n",
    "with open(human_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    human_summary = f.read()\n",
    "\n",
    "with open(ai_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ai_summary = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96UJWz4Ng5ye",
   "metadata": {
    "id": "96UJWz4Ng5ye"
   },
   "source": [
    "### **Step 4: Load the ROUGE metric**\n",
    "\n",
    "*   Load the ROUGE metric.\n",
    "*   Compute the ROUGE scores for the summary.\n",
    "*   Print the scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Hdj1QFyBhM6W",
   "metadata": {
    "id": "Hdj1QFyBhM6W"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and normalizes text for better ROUGE evaluation.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")  # Remove newlines\n",
    "    text = text.lower().strip()  # Convert to lowercase & remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Preprocess all texts\n",
    "document_text = preprocess_text(document_text)\n",
    "human_summary = preprocess_text(human_summary)\n",
    "ai_summary = preprocess_text(ai_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd311cca-bfbc-4d90-a0af-1c049c2f67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE metric\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "# Compute ROUGE scores for Human Summary\n",
    "human_scores = metric.compute(predictions=[human_summary], references=[document_text])\n",
    "\n",
    "# Compute ROUGE scores for AI-Generated Summary\n",
    "ai_scores = metric.compute(predictions=[ai_summary], references=[document_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06cb86b8-8b3c-4cc6-b94c-8c096f440f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ROUGE Score Comparison\n",
      "           Human Summary  AI Summary\n",
      "rouge1            0.1028      0.0932\n",
      "rouge2            0.0774      0.0324\n",
      "rougeL            0.0740      0.0578\n",
      "rougeLsum         0.0740      0.0578\n"
     ]
    }
   ],
   "source": [
    "# Convert ROUGE scores to DataFrame\n",
    "human_scores_df = pd.DataFrame(human_scores, index=[\"Human Summary\"]).T.round(4)\n",
    "ai_scores_df = pd.DataFrame(ai_scores, index=[\"AI Summary\"]).T.round(4)\n",
    "\n",
    "# Combine both scores into a single DataFrame\n",
    "comparison_df = pd.concat([human_scores_df, ai_scores_df], axis=1)\n",
    "\n",
    "# Display ROUGE score comparison in a table format\n",
    "print(\"\\n ROUGE Score Comparison\")\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a6d3d-979a-4082-a8ab-979654dfeb4e",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "The ROUGE score output shows the F-measure for different versions of the ROUGE metric: ROUGE-1, ROUGE-2, and ROUGE-L. These scores provide a measure of how well the summary matches the reference document. The higher the score (closer to 1), the better the match between the summary and the original text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
