{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf292b-5358-4022-ae08-9faf2cd92f5f",
   "metadata": {},
   "source": [
    "# **Text Processing and Feature Extraction in NLP**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd64c2a-c887-4c36-85c9-e61e4104a160",
   "metadata": {},
   "source": [
    "This guide walks through various text processing techniques in NLP, including tokenization, vectorization, one-hot encoding, and word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1de0b-99ca-4a13-9bb5-ff7115919417",
   "metadata": {},
   "source": [
    "**Steps to be followed:**\n",
    "1. Tokenization\n",
    "2. Bag of Words (BoW) Vectorization\n",
    "3. One-Hot Encoding\n",
    "4. Word Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540af60f-ff79-4fba-9522-e9960f5726de",
   "metadata": {},
   "source": [
    "## __Step 1: Tokenization__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d35dc8-217d-4873-8a99-a35048d89b10",
   "metadata": {},
   "source": [
    "What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens), such as words or subwords.\n",
    "This helps in text analysis, NLP preprocessing, and training ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b38c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"AI is transforming the world.\",\n",
    "    \"Natural language processing is a subset of AI.\",\n",
    "    \"Deep learning and machine learning are popular AI techniques.\",\n",
    "    \"AI applications are diverse and growing rapidly.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b9dd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['AI', 'is', 'transforming', 'the', 'world', '.'],\n",
       " ['Natural', 'language', 'processing', 'is', 'a', 'subset', 'of', 'AI', '.'],\n",
       " ['Deep',\n",
       "  'learning',\n",
       "  'and',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'are',\n",
       "  'popular',\n",
       "  'AI',\n",
       "  'techniques',\n",
       "  '.'],\n",
       " ['AI', 'applications', 'are', 'diverse', 'and', 'growing', 'rapidly', '.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sentence) for sentence in text_data]\n",
    "tokenized_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab980f-646b-47f1-ab44-3873a356e9f6",
   "metadata": {},
   "source": [
    "## __Step 2: Bag of Words (BoW) Vectorization__\n",
    "\n",
    "What is BoW?\n",
    "\n",
    "Bag of Words (BoW) converts text into numerical form.\n",
    "It counts word occurrences across sentences/documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae11efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ai', 'and', 'applications', 'are', 'deep', 'diverse', 'growing',\n",
       "        'is', 'language', 'learning', 'machine', 'natural', 'of',\n",
       "        'popular', 'processing', 'rapidly', 'subset', 'techniques', 'the',\n",
       "        'transforming', 'world'], dtype=object),\n",
       " array([[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "vectorized_data = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to array for better visualization\n",
    "vector_array = vectorized_data.toarray()\n",
    "\n",
    "feature_names, vector_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7214cdd-7284-4a05-ae8e-74c53b6b9b0c",
   "metadata": {},
   "source": [
    "## __Step 3: One-Hot Encoding__\n",
    "\n",
    "What is One-Hot Encoding?\n",
    "\n",
    "One-Hot Encoding represents text as binary vectors.\n",
    "Each word gets a unique index and is encoded as 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3cf80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['are',\n",
       "  'learning',\n",
       "  'popular',\n",
       "  'deep',\n",
       "  'a',\n",
       "  'techniques',\n",
       "  'applications',\n",
       "  'machine',\n",
       "  'processing',\n",
       "  '.',\n",
       "  'and',\n",
       "  'of',\n",
       "  'is',\n",
       "  'language',\n",
       "  'subset',\n",
       "  'natural',\n",
       "  'rapidly',\n",
       "  'transforming',\n",
       "  'the',\n",
       "  'ai',\n",
       "  'growing',\n",
       "  'diverse',\n",
       "  'world'],\n",
       " array([[1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "         1., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 2., 1., 0., 0., 1.,\n",
       "         0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Ensure NLTK punkt is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample Text Dataset\n",
    "text_data = [\n",
    "    \"AI is transforming the world.\",\n",
    "    \"Natural language processing is a subset of AI.\",\n",
    "    \"Deep learning and machine learning are popular AI techniques.\",\n",
    "    \"AI applications are diverse and growing rapidly.\"\n",
    "]\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "\n",
    "# Flatten the list of tokenized words\n",
    "flattened_tokens = [word for sentence in tokenized_data for word in sentence]\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = list(set(flattened_tokens))\n",
    "\n",
    "# Initialize OneHotEncoder with the vocabulary\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoder.fit(np.array(vocabulary).reshape(-1, 1))\n",
    "\n",
    "# Encode each sentence\n",
    "one_hot_encoded_data = []\n",
    "for sentence in tokenized_data:\n",
    "    encoded_sentence = encoder.transform(np.array(sentence).reshape(-1, 1))\n",
    "    one_hot_encoded_data.append(np.sum(encoded_sentence, axis=0))\n",
    "\n",
    "# Output the vocabulary and one-hot encoded data\n",
    "vocabulary, np.array(one_hot_encoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab24350",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3221fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa4d53-b1c3-4bb5-b941-d7e6664fa828",
   "metadata": {},
   "source": [
    "## __Step 4: Word Embeddings (Word2Vec)__\n",
    "\n",
    "What is Word2Vec?\n",
    "\n",
    "Word2Vec converts words into dense vector representations.\n",
    "Similar words have similar vectors based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61573d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['ai',\n",
       "  '.',\n",
       "  'is',\n",
       "  'are',\n",
       "  'and',\n",
       "  'learning',\n",
       "  'a',\n",
       "  'transforming',\n",
       "  'the',\n",
       "  'world',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'rapidly',\n",
       "  'subset',\n",
       "  'growing',\n",
       "  'deep',\n",
       "  'machine',\n",
       "  'popular',\n",
       "  'techniques',\n",
       "  'applications',\n",
       "  'diverse',\n",
       "  'of'],\n",
       " array([-5.3929625e-04,  2.3846110e-04,  5.1058391e-03,  9.0093408e-03,\n",
       "        -9.2996191e-03, -7.1191201e-03,  6.4605977e-03,  8.9770071e-03,\n",
       "        -5.0151777e-03, -3.7672531e-03,  7.3795291e-03, -1.5375089e-03,\n",
       "        -4.5374273e-03,  6.5554040e-03, -4.8583425e-03, -1.8160876e-03,\n",
       "         2.8794403e-03,  9.9324819e-04, -8.2843015e-03, -9.4504934e-03,\n",
       "         7.3131216e-03,  5.0689015e-03,  6.7586694e-03,  7.6146907e-04,\n",
       "         6.3525592e-03, -3.4040422e-03, -9.4959326e-04,  5.7699587e-03,\n",
       "        -7.5216787e-03, -3.9335201e-03, -7.5099799e-03, -9.3070691e-04,\n",
       "         9.5371082e-03, -7.3208129e-03, -2.3332399e-03, -1.9356081e-03,\n",
       "         8.0778319e-03, -5.9298510e-03,  4.8323451e-05, -4.7512641e-03,\n",
       "        -9.6004633e-03,  5.0062262e-03, -8.7614404e-03, -4.3917079e-03,\n",
       "        -3.7209342e-05, -2.9563144e-04, -7.6590404e-03,  9.6131861e-03,\n",
       "         4.9816617e-03,  9.2348615e-03, -8.1535587e-03,  4.4938694e-03,\n",
       "        -4.1376026e-03,  8.2439958e-04,  8.5006570e-03, -4.4607469e-03,\n",
       "         4.5206840e-03, -6.7847292e-03, -3.5467541e-03,  9.4015617e-03,\n",
       "        -1.5829535e-03,  3.2085573e-04, -4.1409000e-03, -7.6842210e-03,\n",
       "        -1.5066152e-03,  2.4711967e-03, -8.8488642e-04,  5.5316552e-03,\n",
       "        -2.7416169e-03,  2.2607008e-03,  5.4558786e-03,  8.3438568e-03,\n",
       "        -1.4516106e-03, -9.2064003e-03,  4.3711225e-03,  5.7053770e-04,\n",
       "         7.4413777e-03, -8.1348437e-04, -2.6373402e-03, -8.7524047e-03,\n",
       "        -8.6118397e-04,  2.8299810e-03,  5.4045343e-03,  7.0523052e-03,\n",
       "        -5.7022409e-03,  1.8587064e-03,  6.0913651e-03, -4.8013874e-03,\n",
       "        -3.1058961e-03,  6.7964233e-03,  1.6325536e-03,  1.9025705e-04,\n",
       "         3.4759028e-03,  2.1465658e-04,  9.6202940e-03,  5.0613643e-03,\n",
       "        -8.9158444e-03, -7.0412839e-03,  9.0079260e-04,  6.3948845e-03],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample Text Dataset\n",
    "text_data = [\n",
    "    \"AI is transforming the world.\",\n",
    "    \"Natural language processing is a subset of AI.\",\n",
    "    \"Deep learning and machine learning are popular AI techniques.\",\n",
    "    \"AI applications are diverse and growing rapidly.\"\n",
    "]\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocabulary = list(model.wv.index_to_key)\n",
    "\n",
    "# Get word embeddings for the vocabulary\n",
    "word_embeddings = {word: model.wv[word] for word in vocabulary}\n",
    "\n",
    "# Example: Get embedding for the word 'ai'\n",
    "embedding_ai = word_embeddings['ai']\n",
    "\n",
    "vocabulary, embedding_ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d5bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
